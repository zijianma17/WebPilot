import logging
from .prompts import *

# ========== basic prompt =========
basic_prompt_without_info = """You are an AI agent tasked with verifying the completion of a web exploration task. \n"""
AGENT_ROLE = "controller"

maintask_prompt = """### Main Task
Here is the main task you need to complete:
{task_content}

"""

# ========== redefine prior_knowledge_prompt =========
def gen_prior_knowledge_prompt(domain):
    return gen_prior_knowledge_prompt_origin(domain, AGENT_ROLE)

# ========== basic prompt with prior knowledge =========
def gen_basic_prompt(task_content, domain, agent_role=AGENT_ROLE, function_description=""):
    return gen_basic_prompt_origin(task_content, domain, agent_role, function_description)


# ==================== is_subtask_stopped ====================
class gen_is_subtask_stopped_prompt:
    criteria_prompt = """### Criteria
Following is the state that should be achieved after correctly completing the task. It will be used to determine whether the task has been successfully completed.
{expectation}
"""
    @staticmethod
    def observation(domain, expectation, actree, observation_description):
        logging.info("===== Currentprompt: gen_is_subtask_stopped_prompt.observation =====")
        # special prompts
        function_description = """You need to verify whether the observation meets the criteria to further decide whether to stop the subtask. \n"""
        observation_description_prompt = f"""### Observation Description
Here is the observation description of the current environment generated by yourself:
{observation_description}

"""
        observation_meets_criteria_prompt = """### Reasoning Process:
Does the information of "Observation" and "Observation Description" meet the "Criteria"? Briefly explain why. Also briefly describe the current state, which could be helpful in the future to let you know where you are. Pay more attention to "Observation Description" than "Observation". Are you in the right page? 

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_observation": string of the reasoning, clearly state whether the "Observation" and "Observation Description" fully meet the "Criteria";
    "observation_meets_criteria": boolean value, False if the "Observation" and "Observation Description" do not fully meet the "Criteria"; True only if all specified elements are present and correct;
}
"""
        prompt = gen_basic_prompt(task_content="", domain=domain, function_description=function_description)
        prompt += observation_prompt.format(actree=actree)
        if observation_description.strip() != "":
            prompt += observation_description_prompt
        prompt += gen_is_subtask_stopped_prompt.criteria_prompt.format(expectation=expectation)
        prompt += observation_meets_criteria_prompt
        prompt += format_instructions_prompt
        return prompt

    @staticmethod
    def actions(domain, expectation, executed_actions):
        logging.info("===== Currentprompt: gen_is_subtask_stopped_prompt.actions =====")

        #special prompts
        function_description = """You need to verify whether the necessary actions required by the criteria have been executed to further decide whether to stop the subtask. \n"""
        necessary_actions_prompt = """### Reasoning Process:
Analyze whether all actions in "Necessary actions" have been completed by comparing them with the "Executed Actions". It is not crucial that each ‘Necessary Action’ exactly matches an ‘Executed Action’; the key is to verify if all actions required by the ‘Necessary Actions’ have been performed as part of the ‘Executed Actions’.
Do NOT assume any action has been completed if it is not explicitly shown in the Executed Actions.
"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_actions": a single STRING of analysis whether each of all necessary actions required by "Necessary Actions" in "Criteria" has been executed, as shown in the "Executed Actions";
    "necessary_actions_executed": boolean value, True if all necessary actions have been executed, also True if no necessary actions are required;
}
"""
        prompt = gen_basic_prompt(task_content="", domain=domain, function_description=function_description)
        prompt += gen_is_subtask_stopped_prompt.criteria_prompt.format(expectation=expectation)
        if executed_actions.strip() != "":
            prompt += executed_action_prompt.format(executed_actions=executed_actions)
        prompt += necessary_actions_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt
        return prompt

    @staticmethod
    def reflection(domain, task_content, expectation, node_reflection):
        logging.info("===== Currentprompt: gen_is_subtask_stopped_prompt.reflection =====")
        
        # special prompts
        function_description = """You need to check the reflection of the current state to further decide whether to stop the subtask. \n"""
        node_reflection_prompt = f"""### Node Reflection
You have executed an action and made the following reflection in the current state:
{node_reflection}

"""
        reflection_indicates_action_prompt = """### Reasoning Process:
Is there any clear advice from the "Node Reflection", for further actions or stopping the task? Are they useful to complete the "Main Task"? Briefly explain why.
Is there a clear suggestion about what to do next? Or is the reflection only confirms the previous actions and does not provide advice for future actions? 

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_reflection": string of the reasoning, whether the "Node Reflection" suggests further actions clearly or is unclear, or suggests stopping;
    "reflection_suggests_further_actions": boolean value, True if the "Node Reflection" suggests further actions clearly, False if it is unclear or suggests stopping;
}
"""
        prompt = gen_basic_prompt(task_content="", domain=domain, function_description=function_description)
        prompt += maintask_prompt.format(task_content=task_content)
        if node_reflection.strip() != "":
            prompt += node_reflection_prompt
        prompt += reflection_indicates_action_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt
        return prompt

    # @staticmethod
    def reasoning(task_content, expectation, actree, executed_actions, ScratchPad_Info, node_reflection, observation_description):
        """
        used for judgment of a subtask is finished or not, the env may note terminated. so the agent has to decide it by itself.
        """
        logging.info("===== Currentprompt: gen_is_subtask_stopped_prompt =====")

        # special prompts
        criteria_prompt = f"""### Criteria
Here is the criteria for finishing the subtask:
{expectation}

"""
        observation_description_prompt = f"""### Observation Description
Here is the observation description of the current environment generated by yourself:
{observation_description}

"""
        node_reflection_prompt = f"""### Node Reflection
You have made the following reflection in the current state:
{node_reflection}

"""        
        is_subtask_stopped_instruction_prompt = """### Reasoning Process:
**Stop Decision**:
Now, you should judge whether to stop the current task. Consider the following aspects:
    1. First, you must fully observe "Observation" and "Observation Description". Does the information above meet the "Criteria"? If so, you can stop the task.
    2. Second, does the "Necessary Actions" in "Criteria" require some actions to be executed? If all necessary actions have been executed, as shown in "Executed Actions", then you can stop the task.
    3. Third, is there any clear advice from the "Node Reflection", for further actions or stopping the task? If there is a clear suggestion about what to do next, you should not stop. But if the reflection only confirms the previous actions and does not provide advice for future actions, you should stop the task.
Evaluate the current situation: Should you stop now? Summarize your reasoning based on the above aspects after the key "reason". Then, indicate whether to halt the task with a True/False response.

**Subtask Completion**: If you decide to stop, judge how well the task has been completed. Does the current state meet the "Criteria"? If so, briefly explain why. If not, what could have been done differently? Also briefly describe the current state, which could be helpful in the future to let you know where you are.

ATTENTION:
If you decide to stop, the "Main Task" will end immediately with no further actions.
The information above is all you have, with no further additional details.
Therefore, do not make assumptions or further inferences(such as "it might be possible" to complete). Judge only based on the information currently available. If the required information from "Criteria" is needed, output it in your response. If you can't, then you should continue the task.

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_observation": string of the reasoning, whether the "Observation" and "Observation Description" meet the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" and "Observation Description" meet the "Criteria";
    "reasoning_of_actions": string of the reasoning, whether the all necessary actions required by "Necessary Actions" in "Criteria" have been executed, as shown in the "Executed Actions";
    "necessary_actions_executed": boolean value, True if all necessary actions have been executed;
    "reasoning_of_reflection": string of the reasoning, whether the "Node Reflection" suggests further actions clearly or is unclear, or suggests stopping;
    "reflection_suggests_further_actions": boolean value, True if the "Node Reflection" suggests further actions clearly, False if it is unclear or suggests stopping;
    "reason": string of the reason for your decision on whether to stop the task;
    "final_answer": string of the final answer if the "Criteria" requires an answer;
    "stop_decision": boolean value of whether to stop the task.
    "subtask_completeness": string of the evaluation of the current state, how well it meets the "Criteria", and a brief description of the current state.
}
"""
        prompt = basic_prompt_without_info
        prompt += observation_prompt.format(actree=actree)
        prompt += criteria_prompt
        if observation_description.strip() != "":
            prompt += observation_description_prompt
        if executed_actions.strip() != "":
            prompt += executed_action_prompt.format(executed_actions=executed_actions)
        if node_reflection.strip() != "":
            prompt += node_reflection_prompt
        prompt += is_subtask_stopped_instruction_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt
        return prompt
    
    @staticmethod
    def judging(reasonings: dict):
        """
        after generating the reasoning, use it to generate the final judging prompt of the boolen flags
        """
        reasonings_str = f"""**reasoning_of_observation**: {reasonings["reasoning_of_observation"]}
**reasoning_of_actions**: {reasonings["reasoning_of_actions"]}
**reasoning_of_reflection**: {reasonings["reasoning_of_reflection"]}
"""
        output_flags = """
    "observation_meets_criteria": boolean value, True if the "reasoning_of_observation" indicates that the observation satisfies the criteria;
    "necessary_actions_executed": boolean value, True if the "reasoning_of_actions" indicates that all necessary actions have been executed;
    "reflection_suggests_further_actions": boolean value, True if the "reasoning_of_reflection" confirms that the Reflection suggests further actions clearly,, False if it is unclear or suggests stopping.
"""
        prompt = gen_basic_judging_prompt(reasonings_str, output_flags)

        return prompt

# ==================== gen_subtask_stop_verifier_prompt ====================
class gen_subtask_stop_verifier_prompt:
    criteria_prompt = """### Criteria
Here is the criteria for finishing your current task:
{expectation}

"""

    @staticmethod
    def observation(domain, actree, observation_description, expectation):
        logging.info("===== Currentprompt: gen_subtask_stop_verifier_prompt.observation =====")

        observation_description_prompt = f"""### Observation Description
{observation_description}

"""
        subtask_verifier_instruction_prompt = """### Reasoning Process:
Does the information from "Observation" and "Observation Description" meet the "Criteria"? 
Pay more attention to "Observation Description" than "Observation". Are you in the right page? 

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "reasoning_of_observation": string of the reasoning, whether the "Observation" and "Observation Description" meet the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" and "Observation Description" meet the "Criteria";
}
"""
        # "subtask_completeness": string of the evaluation of the current state, how well it meets the "Criteria".
        # "complete_flag": boolean value of whether you think you have finished the task.
        prompt = gen_basic_prompt(task_content="", domain=domain) # set task_content to empty 
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += gen_subtask_stop_verifier_prompt.criteria_prompt.format(expectation=expectation)
        prompt += subtask_verifier_instruction_prompt
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def actions(domain, expectation, executed_actions):
        logging.info("===== Currentprompt: gen_subtask_stop_verifier_prompt.actions =====")

        subtask_verifier_instruction_prompt = """### Reasoning Process:
Does the "Criteria" have "Necessary Actions" and each necessary action is executed as shown in "Executed Actions"? Analyze each necessary action and evaluate whether it has been executed. Briefly explain why. 

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "reasoning_of_actions": string of the reasoning, whether the all necessary actions required by the "Criteria" have been executed, as shown in the "Executed Actions";
    "necessary_actions_executed": boolean value, True if all necessary actions have been executed;
}
"""
        prompt = gen_basic_prompt(task_content="", domain=domain) # set task_content to empty 
        if executed_actions.strip() != "":
            prompt += executed_action_prompt.format(executed_actions=executed_actions)
        prompt += gen_subtask_stop_verifier_prompt.criteria_prompt.format(expectation=expectation)
        prompt += subtask_verifier_instruction_prompt
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def reasoning(task_content, domain, actree, observation_description, expectation, executed_actions):
        """
        ask one more time to verify the stop decision
        """
        logging.info("===== Currentprompt: gen_subtask_stop_verifier_prompt =====")

        # special prompts
        criteria_prompt = f"""### Criteria
Here is the criteria for finishing your current task:
{expectation}

Judge whether the task is finished based on the criteria above.

"""
        observation_description_prompt = f"""### Observation Description
{observation_description}

"""
        subtask_verifier_instruction_prompt = """### Reasoning Process:
**Stop Decision**:
Judge whether to stop the current task. Consider the following aspects:
    1. First, does the information from "Observation" and "Observation Description" meet the "Criteria"? Pay more attention to "Observation Description" than "Observation". Are you in the right page? If you cannot get a confirmation or denial, then think according to the following guidelines. Note that most of time, the main body of the webpage is the most important part. Don't be misled by the sidebar or the top bar.
    2. Second, if the info from observation is not enough, does the "Criteria" require some actions to be executed? If all necessary actions have been executed, as shown in "Executed Actions", then you can stop the task.
Evaluate the current situation: Should you stop now? Summarize your reasoning based on the above aspects after the key "reason". Then, indicate whether to stop the task with a True/False response.

ATTENTION:
If you decide to stop, the "Main Task" will end immediately with no further actions.
The information above is all you have, with no further additional details.
Therefore, do not make assumptions or further inferences(such as "it might be possible" to complete). Judge only based on the information currently available. If the required information from "Criteria" is needed, output it in your response. If you can't, then you should continue the task.

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "reasoning_of_observation": string of the reasoning, whether the "Observation" and "Observation Description" meet the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" and "Observation Description" meet the "Criteria";
    "reasoning_of_actions": string of the reasoning, whether the all necessary actions required by the "Criteria" have been executed, as shown in the "Executed Actions";
    "necessary_actions_executed": boolean value, True if all necessary actions have been executed;
    "reason": string of the reason for your decision on whether to stop the task;
    "stop_decision": boolean value of whether to stop the task.
}
"""
        # "subtask_completeness": string of the evaluation of the current state, how well it meets the "Criteria".
        # "complete_flag": boolean value of whether you think you have finished the task.
        prompt = gen_basic_prompt(task_content="", domain=domain) # set task_content to empty 
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        if executed_actions.strip() != "":
            prompt += executed_action_prompt.format(executed_actions=executed_actions)
        prompt += criteria_prompt
        prompt += subtask_verifier_instruction_prompt
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def judging(reasonings: dict):
        """
        after generating the reasoning, use it to generate the final judging prompt of the boolen flags
        """
        reasonings_str = f"""**reasoning_of_observation**: {reasonings["reasoning_of_observation"]}
**reasoning_of_actions**: {reasonings["reasoning_of_actions"]}
""" 
        output_flags = """
    "observation_meets_criteria": boolean value, True if the "reasoning_of_observation" indicates that the observation satisfies the criteria;
    "necessary_actions_executed": boolean value, True if the "reasoning_of_actions" indicates that all necessary actions have been executed;
"""
        prompt = gen_basic_judging_prompt(reasonings_str, output_flags)

        return prompt

# ==================== gen_subtask_completeness_estimator_prompt ====================
class gen_subtask_completeness_estimator_prompt:
    criteria_prompt = """### Criteria
Following is the state that should be achieved after correctly completing the task. It will be used to determine whether the task has been successfully completed.
{expectation}
"""
    # observation
    @staticmethod
    def observation(task_content, domain, expectation, actree, observation_description,):
        logging.info("===== Currentprompt: gen_subtask_completeness_estimator_prompt.observation =====")
        
        # special prompts
        function_description = """You need to verify whether the observation meets the criteria and further decide whether the subtask is completed. \n"""
        observation_description_prompt = f"""### Observation Description
Here is a description generated as a summary of the current state of the webpage:
{observation_description}
"""
        completion_instruction_prompt = f"""### Reasoning Process
**Observation Analysis**:
- Fully observe the "Observation" and "Observation Description". Summarize the current state in your own words and in a short sentence after the key "observation_analysis".

**Completeness**:
Does the information on the "Observation" and "Observation Description" meet the "Criteria"? If yes, briefly explain why. If not, what is missing?
Focus on the "Main Task", How well is it completed? Output your conclusion after the key "task_completeness". Then give your final decision: is the task successfully completed? Output True/False after the key "complete_flag".

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "observation_analysis": string of your summary of the current state in a short sentence;
    "task_completeness": string of the evaluation of how well the "Observation" and "Observation Description" meet the "Criteria";
    "complete_flag": boolean value of whether you think you have finished the task.
}
""" 
        prompt = gen_basic_prompt("", domain=domain, function_description=function_description) 
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_subtask_completeness_estimator_prompt.criteria_prompt.format(expectation=expectation)
        prompt += completion_instruction_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt
    
    # actions
    @staticmethod
    def actions(task_content, domain, expectation, exeucted_actions, actions_n_refs_for_sib,):
        logging.info("===== Currentprompt: gen_subtask_completeness_estimator_prompt.actions =====")
        
        # special prompts
        function_description = """You need to verify whether the necessary actions required by the criteria have been executed to further decide whether the subtask is completed. \n"""
        reflections_for_sib_prompt = f"""### Executed Actions and Reflections:
Here are the executed actions, for each action, you have also made a reflection on what could be done differently:
{actions_n_refs_for_sib}

"""
        completion_instruction_prompt = f"""### Reasoning Process
**Actions Analysis**:
Analyze whether all actions in "Necessary actions" have been completed by comparing them with the "Executed Actions". It is not crucial that each ‘Necessary Actions’ exactly matches an ‘Executed Action’; the key is to verify if all actions required by the ‘Necessary Actions’ have been performed as part of the ‘Executed Actions’. Output your analysis after the key "action_analysis".
Do NOT assume any action has been completed if it is not explicitly shown in the Executed Actions.

**Completeness**:
Focus on the "Main Task": "{task_content}". How well is it completed?
Besides the requirement of the "Criteria", what else has this task achieved? Describe the effects.
Output your conclusion after the key "task_completeness". Then give your final decision: is the task successfully completed? Output True/False after the key "complete_flag".

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "action_analysis": a single STRING of analysis whether each of all necessary actions required by "Necessary Actions" in "Criteria" has been executed, as shown in the "Executed Actions";
    "task_completeness": string of the evaluation of how well the "Executed Actions" meet the "Criteria", what else has been achieved;
    "complete_flag": boolean value of whether you think you have finished the task.
}
""" 
        prompt = gen_basic_prompt("", domain=domain, function_description=function_description) 
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_subtask_completeness_estimator_prompt.criteria_prompt.format(expectation=expectation)
        prompt += executed_action_prompt.format(executed_actions=exeucted_actions)
        prompt += completion_instruction_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt
    
    # sub_reflection
    @staticmethod
    def sub_reflection(task_content, domain, expectation, former_actree, obs_completeness, action_completness, executed_actions):
        logging.info("===== Currentprompt: gen_subtask_completeness_estimator_prompt.sub_reflection =====")

        # special prompts
        function_description = """You have not finished a task, you need to make a reflection on the current task to help yourself to redo the task. \n"""
        former_observation_prompt = f"""### Former Observation
Here is the observation of the former state, where you started the current task:
{former_actree}

"""
        task_completeness = obs_completeness if obs_completeness.strip() != "" else action_completness
        completeness_prompt = f"""### Task Completeness Analysis
Here is the analysis of the task completeness:
{task_completeness}
"""
        
        # observation
        if obs_completeness.strip() != "":
            completion_instruction_prompt = """### Reasoning Process
You have not finished the "Main Task", as you didn’t reach the right page as expected in the "Criteria". Reflect on the executed actions and analyze why these actions did not successfully complete the "Main Task" or meet the "Criteria". If redoing this task, consider the following aspects for improvement:
    1. Review the "Task Completeness Analysis" and identify how the "Executed Actions" led to the current state. Analyze the reasons for any errors and determine where the deviation from the correct path occurred.
	2. Return to the state before, i.e., the "Former Observation," and compare it with the "Main Task" to identify what could be done differently to achieve the desired outcome.
Output your reflection after the key "task_reflection". Try to make it within 50 words.

"""
            format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "fail_reason": a short string of the reason why the task was not completed successfully, where the error occurred and point out where the deviation from the correct path occurred;
    "task_reflection": string of the reflection on the current task, if you could redo the task, what could be done differently or more effectively. Highlight any specific changes or improvements for successfully completing the task. This reflection will be used as guidance for reattempting the task.
}
"""     
        # action
        elif action_completness.strip() != "":
            completion_instruction_prompt = """### Reasoning Process
You have not finished the "Main Task", as you didn't conduct all the necessary actions as required in the "Criteria". Reflect on the executed actions and analyze why these actions did not successfully complete the "Main Task" or meet the "Criteria". If redoing this task, consider the following aspects for improvement:
    1. Review the "Task Completeness Analysis" and identify how the "Executed Actions" led to the current state. Analyze the reasons for any errors and determine where the deviation from the correct path occurred.
    2. Is it due to the wrong order of actions or the wrong action itself?
    3. Return to the state before, i.e., the "Former Observation," and compare it with the "Main Task" to identify what could be done differently to achieve the desired outcome.
Output your reflection after the key "task_reflection".

"""
            format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "fail_reason": a short string of the reason why the task was not completed successfully, where the error occurred and point out where the deviation from the correct path occurred;
    "task_reflection": string of the detailed reflection on the current task, focusing on what could be done differently or more effectively. Highlight any specific changes or improvements for successfully completing the task. This reflection will be used as guidance for reattempting the task. Try to make it within 50 words.
}
""" 
        prompt = gen_basic_prompt("", domain=domain, function_description=function_description) 
        prompt += former_observation_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_subtask_completeness_estimator_prompt.criteria_prompt.format(expectation=expectation)
        prompt += completeness_prompt
        prompt += executed_action_prompt.format(executed_actions=executed_actions)
        prompt += completion_instruction_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt
    
    @staticmethod
    def sub_reflection_old(task_content, domain, expectation, former_actree, observation_analysis, action_analysis):
        logging.info("===== Currentprompt: gen_subtask_completeness_estimator_prompt.sub_reflection =====")
        
        # special prompts
        function_description = """You need to give a final assessment of the subtask completion or give a reflection on the current task if you think it is not finished. \n"""
        former_observation_prompt = f"""### Former Observation
Here is the observation of the former state, where you started the current task:
{former_actree}

"""
        observation_analysis = f"""### Observation Analysis
{observation_analysis}
"""
        action_analysis_prompt = f"""
### Action Analysis
{action_analysis}
"""
        if action_analysis.strip() != "":
            completion_instruction_prompt = """### Reasoning Process
**Completeness**: You already stopped, judge how well the task has been completed. 
Consider the "Observation Analysis" and "Action Analysis". Do them satisfy the "Criteria"? Summary them and output your analysis after the key "subtask_completeness". Then give your final conclusion: is the task successfully completed? Output True/False after the key "complete_flag".

**Task Reflection**: If you think you have not finished the task, reflect on what you have done. Consider the following aspects:
    1. Consider the "Observation Analysis", did you reach the right page as expected in the "Criteria"? 
    2. Is there advices from "Action Analysis"? 
    3. You will return to the state before, i.e. the "Former Observation", compare with the "Main Task", what could be done differently to be more reasonable?
Output your reflection after the key "task_reflection".

"""

            format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "task_completeness": string of the evaluation of how well the "Observation Analysis" and "Action Analysis" meet the "Criteria";
    "complete_flag": boolean value of whether you think you have finished the task.
    "task_reflection": string of the reflection on the current task, what could be done differently or what could be more reasonable.
}
""" 
        else: # no action_analysis
            completion_instruction_prompt = """### Reasoning Process
**Completeness**: You already stopped, judge how well the task has been completed. 
Consider the "Observation Analysis". Does it satisfy the "Criteria"? 
Beside the reuirement of the "Criteria", what else has this task achieved? Describe the effects.
Summary them and output your analysis after the key "subtask_completeness". 
Then give your final conclusion: is the task successfully completed? Output True/False after the key "complete_flag".

**Task Reflection**: If you think you have not finished the task, reflect on what you have done. Consider the following aspects:
    1. Consider the "Observation Analysis", did you reach the right page as expected in the "Criteria"? 
    2. You will return to the state before, i.e. the "Former Observation", compare with the "Main Task", what could be done differently to be more reasonable?
Output your reflection after the key "task_reflection".

"""

            format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "task_completeness": string of the evaluation of how well the "Observation Analysis" and "Action Analysis" meet the "Criteria", what else has been achieved;
    "complete_flag": boolean value of whether you think you have finished the task.
    "task_reflection": string of the reflection on the current task, what could be done differently or what could be more reasonable.
}
""" 
        prompt = gen_basic_prompt("", domain=domain, function_description=function_description) 
        prompt += former_observation_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_subtask_completeness_estimator_prompt.criteria_prompt.format(expectation=expectation)
        prompt += observation_analysis
        if action_analysis.strip() != "":
            prompt += action_analysis_prompt
        prompt += completion_instruction_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def reasoning(task_content, domain, former_actree, actree, observation_description, expectation, 
                                                actions_n_refs_for_sib,):
        """
        esimate the completion of a subtask.  
        """
        logging.info("===== Currentprompt: gen_subtask_completeness_estimator_prompt =====")
        # special prompts
        criteria_prompt = f"""### Criteria
Here is the criteria for finishing your current task:
{expectation}

Judge how the task is completed based on the criteria above.

"""
        former_observation_prompt = f"""### Former Observation
Here is the observation of the former state, where you started the current task:
{former_actree}

"""
        observation_description_prompt = f"""### Observation Description
{observation_description}

"""
        reflections_for_sib_prompt = f"""### Executed Actions and Reflections:
{actions_n_refs_for_sib}

"""
        completion_instruction_prompt = """### Reasoning Process
**Completeness**: You already stopped, judge how well the task has been completed. Consider the following aspects:
    1. First, fully observe "Observation" and "Observation Description". Does the information on it meet the "Criteria"? Pay more attention to "Observation Description" than "Observation". The main body of the webpage is the most important part. Don't be misled by the sidebar or the top bar. If yes, briefly explain why. If not, the task is not completed.
    2. Second, does the "Criteria" require some specific actions to be executed? If the "Executed Actions" do not contain all necessary actions, the task is not completed.
Output your analysis after the key "subtask_completeness". Then give your final conclusion: is the task successfully completed? Output True/False after the key "complete_flag".

**Subtask Reflection**: If you think you have not finished the task, reflect on what you have done. Consider the following aspects:
    1. Compare the "Former Observation" and the "Observation" of the current state, did you reach the right page as expected in the "Criteria"? If not, what could have been done differently?
    2. Focus on the current "Observation", is the current "Subtask" not actually achievable? For example the page of the "Criteria" does not exist, or the filter you want to use is not available. What could be more reasonable?
    3. You might found it impossible to complete the task midway. Is there advices in the "Executed Actions and Reflections"?
    4. You will return to the state before, i.e. the "Former Observation", compare with the "Main Task", what could be more reasonable?
Output your reflection after the key "task_reflection".

"""

        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "reasoning_of_observation": string of your reasoning, whether the info of "Observation" and "Observation Description" meets the "Criteria";
    "observation_meets_criteria": boolean value, True if the info of "Observation" and "Observation Description" meets the "Criteria";
    "reasoning_of_actions": string of your reasoning, whether all necessary actions of "Criteria" have been executed in the "Executed Actions";
    "necessary_actions_executed": boolean value, True if all necessary actions of "Criteria" have been executed in the "Executed Actions";
    "subtask_completeness": string of the evaluation of the current state, how well it meets the "Criteria".
    "complete_flag": boolean value of whether you think you have finished the task.
    "task_reflection": string of the reflection on the current subtask, what could be done differently or what could be more reasonable.
}
""" 
        prompt = gen_basic_prompt("", domain=domain) 
        prompt += former_observation_prompt
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += criteria_prompt
        if actions_n_refs_for_sib.strip() != "":
            prompt += reflections_for_sib_prompt
        prompt += completion_instruction_prompt
        prompt += format_instructions_prompt

        return prompt
    
    @staticmethod
    def judging(reasonings: dict):
        """
        after generating the reasoning, use it to generate the final judging prompt of the boolen flags
        """
        reasonings_str = f"""**reasoning_of_observation**: {reasonings["reasoning_of_observation"]}
**reasoning_of_actions**: {reasonings["reasoning_of_actions"]}
"""

        output_flags = """
    "observation_meets_criteria": boolean value, True if the "reasoning_of_observation" indicates that the observation satisfies the criteria;
    "necessary_actions_executed": boolean value, True if the "reasoning_of_actions" indicates that all necessary actions have been executed;
"""

        prompt = gen_basic_judging_prompt(reasonings_str, output_flags)

        return prompt 
    
    @staticmethod
    def forced_terminal(task_content, domain, former_actree, actree, observation_description, expectation, actions_n_refs_for_sib,):
        """
        for forced terminal nodes. The agent is informed that the task is incomplete. Analyze reason and reflection.
        """
        logging.info("===== Currentprompt: gen_subtask_completeness_estimator_prompt.forced_terminal =====")
        
        # special prompts
        criteria_prompt = f"""### Criteria
Here is the criteria for finishing your current task:
{expectation}

Judge how the task is completed based on the criteria above.

"""
        former_observation_prompt = f"""### Former Observation
Here is the observation of the former state, where you started the current task:
{former_actree}

"""
        observation_description_prompt = f"""### Observation Description
{observation_description}

"""
        reflections_for_sib_prompt = f"""### Executed Actions and Reflections:
{actions_n_refs_for_sib}

"""
        forced_terminal_instruction_prompt = """### Reasoning Process
**Completeness**: You have not completed the "Main Task". Analyze the reason for incompleteness. Consider the following aspects:
    1. First, fully observe "Observation" and "Observation Description". Does the information on it meet the "Criteria"? Pay more attention to "Observation Description" than "Observation". The main body of the webpage is the most important part. Don't be misled by the sidebar or the top bar. How does the current "Observation" differ from the "Criteria"?
    2. Second, does the "Criteria" require some specific actions to be executed? What actions are missing in the "Executed Actions"?
Output your analysis after the key "subtask_completeness".

**Subtask Reflection**: Reflect on what you have done. Consider the following aspects:
    1. Compare the "Former Observation" and the "Observation" of the current state, did you reach the right page as expected in the "Criteria"? If not, what could have been done differently?
    2. Does the "Executed actions and Reflections" provide any advice for different actions?
    3. You will return to the state of the beginning, i.e. the "Former Observation", consider the "Main Task", what could be more reasonable to do?
Output your reflection after the key "task_reflection".
"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{   
    "reasoning_of_observation": string of your reasoning, whether the info of "Observation" and "Observation Description" meets the "Criteria";
    "reasoning_of_actions": string of your reasoning, whether all necessary actions of "Criteria" have been executed in the "Executed Actions";
    "subtask_completeness": string of the evaluation of the current state, how well it meets the "Criteria".
    "task_reflection": string of the reflection on the current subtask, what could be done differently or what could be more reasonable.
}
"""
        prompt = gen_basic_prompt("", domain=domain) 
        prompt += former_observation_prompt
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += criteria_prompt
        if actions_n_refs_for_sib.strip() != "":
            prompt += reflections_for_sib_prompt
        prompt += forced_terminal_instruction_prompt
        prompt += format_instructions_prompt

        return prompt

# ==================== is_webtask_stopped ====================
class gen_is_webtask_stopped_prompt:
    criteria_prompt = """### Criteria
Following is the state that should be achieved after correctly completing the task. It will be used to determine whether the task has been successfully completed.
{expectation}
"""
    # finished_subtasks
    @staticmethod
    def finished_subtasks(task_content, domain, expectation, finished_subtasks,):
        logging.info("===== Currentprompt: gen_is_webtask_stopped_prompt.finished_subtasks =====")

        # special prompts
        function_description = """You need to verify whether the finished subtasks are enough to fulfill the main task and its criteria. \n"""
        if finished_subtasks.strip() == "":
            finished_subtasks = """Currently, there are no finished subtasks, meaning that you have just finished the first subtask."""
        finished_subtasks_prompt = f"""### Finished Subtasks
You have already finished the following subtasks, with their corresponding completeness:
{finished_subtasks}

"""
        final_stop_decision_prompt = """### Reasoning Process
Are the "Finished Subtasks" enough to fulfill the "Main Task" and its "Criteria"? If yes, explain why. If not, analyze what else is needed to be done to finish the task.
"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_finished_subtasks": string of your reasoning, whether "Finished Subtasks" are enough to fulfill the "Main Task" and its "Criteria";
    "finished_subtasks_sufficient": boolean value, True if "Finished Subtasks" are enough to fulfill the "Main Task" and its "Criteria";
}
"""

        prompt = gen_basic_prompt("", domain, function_description=function_description)
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_is_webtask_stopped_prompt.criteria_prompt.format(expectation=expectation)
        prompt += finished_subtasks_prompt
        prompt += final_stop_decision_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt
    
    # plan
    @staticmethod
    def plan(task_content, domain, expectation, plan, finished_subtasks,):
        logging.info("===== Currentprompt: gen_is_webtask_stopped_prompt.plan =====")

        # special prompts
        function_description = """You need to verify whether the rough plan is still necessary to fulfill the main task and its criteria. \n"""
        if plan.strip() == "":
            plan = """Currently, there are no subtasks in the "Rough Plan", meaning that you have just finished the last subtask."""
        rough_plan_prompt = f"""### Rough Plan
{plan}
It's not guaranteed that all the subtasks in the "Rough Plan" are necessary. So it's only for your reference.

"""
        if finished_subtasks.strip() == "":
            finished_subtasks = """Currently, there are no finished subtasks, meaning that you have just finished the first subtask."""
        finished_subtasks_prompt = f"""### Finished Subtasks
You have already finished the following subtasks, with their corresponding completeness:
{finished_subtasks}

"""
        final_stop_decision_prompt = """### Reasoning Process
Are all the subtasks in the "Rough Plan" necessary or not to finish the "Main Task"? 
Is the expected effect of the "Rough Plan" achieved by the "Finished Subtasks"?
If the "Criteria" does not require an answer, then some tasks starting with "Check", "Observe", "Locate", are very likely to be unnecessary.
Analyze the necessity of each subtask in the "Rough Plan".

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_plan": string of your reasoning, analysis of whether each subtask in "Rough Plan" is necessary or not;
    "rough_plan_is_necessary": boolean value, True if at least one subtask in the "Rough Plan" is necessary, False if all the subtasks in the "Rough Plan" are unnecessary, or there is no subtask in the "Rough Plan";
}
    """
        prompt = gen_basic_prompt("", domain, function_description=function_description)
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_is_webtask_stopped_prompt.criteria_prompt.format(expectation=expectation)
        prompt += finished_subtasks_prompt
        if plan.strip() != "":
            prompt += rough_plan_prompt
        prompt += final_stop_decision_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt
    
    # observation
    @staticmethod
    def observation(task_content, domain, expectation, actree, observation_description,):
        logging.info("===== Currentprompt: gen_is_webtask_stopped_prompt.observation =====") 

        # special prompts
        function_description = """You need to verify whether the observation meets the criteria to further decide whether the "Main Task" is completed. \n"""
        observation_description_prompt = f"""### Observation Description
Here is the observation description of the current environment generated by yourself:
{observation_description}

"""
        final_stop_decision_prompt = """### Reasoning Process
Does the "Observation Description" fulfills the "Target Page Description" in the "Criteria"? If yes, explain why. If not, analyze what could be done to meet the "Criteria".

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_observation": string of your reasoning, whether the current "Observation" meets the "Target Page Description" in the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" meets the "Criteria";
}
"""
        prompt = gen_basic_prompt("", domain, function_description=function_description)
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_is_webtask_stopped_prompt.criteria_prompt.format(expectation=expectation)
        prompt += final_stop_decision_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt
    
    # answer
    @staticmethod
    def answer(task_content, domain, expectation, actree, final_answer,):
        logging.info("===== Currentprompt: gen_is_webtask_stopped_prompt.answer =====")

        # special prompts
        function_description = """You need to verify whether the final answer meets the criteria to further decide whether the "Main Task" is completed. \n"""
        final_answer_prompt = f"""### Final Answer
Here is the final answer you have generated:
{final_answer}

"""
        final_stop_decision_prompt = """### Reasoning Process
Compare the "Final Answer" with the "Criteria". Consider the information from "Observation". 
Does the "Final Answer" meet the "Answer Requirements" in the "Criteria"?
Output your analysis after the key "reasoning_of_answer". Then output your answer after the key "final_answer".

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_answer": string of your reasoning, whether the answer provided is generated properly from the "Observation" and meets the "Criteria";
    "answer_meets_criteria": boolean value, True if the answer meets the "Criteria";
    "final_answer": string of the final answer if the "Criteria" requires an answer;
}
"""
        prompt = gen_basic_prompt("", domain, function_description=function_description)
        prompt += observation_prompt.format(actree=actree)
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_is_webtask_stopped_prompt.criteria_prompt.format(expectation=expectation)
        prompt += final_answer_prompt
        prompt += final_stop_decision_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def reasoning(task_content, domain, actree, observation_description, plan, finished_subtasks, expectation, ScratchPad_Info, terminal_node_reflection,
                                    need_answer_flag,
                                    last_subtask_is_info_extraction_flag,
                                    ):
        """
        used for judgment of a webtask is finished or not, the env may note terminated. so the agent has to decide it by itself.
        """
        logging.info("===== Currentprompt: gen_is_webtask_stopped_prompt =====")

        # special prompts
        if plan.strip() == "":
            plan = """Currently, there are no subtasks in the "Rough Plan", meaning that you have just finished the last subtask."""
        rough_plan_prompt = f"""### Rough Plan    
Here is the rough plan you have generated, which are intended to be done after the current subtask:
{plan}
It's not guaranteed that all the subtasks in the "Rough Plan" are necessary. So it's only for your reference.

"""
        observation_description_prompt = f"""### Observation Description
Here is the observation description of the current environment generated by yourself:
{observation_description}

"""
        if finished_subtasks.strip() == "":
            finished_subtasks = """Currently, there are no finished subtasks, meaning that you have just finished the first subtask."""
        finished_subtasks_prompt = f"""### Finished Subtasks
You have already finished the following subtasks, with their corresponding completeness:
{finished_subtasks}

"""
        criteria_prompt = f"""### Criteria
Here is the criteria for finishing the "Main Task":
{expectation}

Judge whether the "Main Task" is finished based on the criteria above.

"""
        node_reflection_prompt = f"""### Node Reflection
You have made the following reflection in the current state:
{terminal_node_reflection}

This reflection may provide some insights for your decision.

"""
        empty_ScratchPad_Info = """### ScratchPad Information
Currently, there is no information in the ScratchPad. If the "Criteria" needs some answer to be stored, it is possiblly you have not found it, which means you cannot stop the task now.

"""
        final_stop_decision_prompt = """### Reasoning Process
**Stop Decision**:
Now, judge whether to stop the current "Main Task":
    1. First, consider the "Finished Subtasks" and the "Rough Plan":
        - Are the "Finished Subtasks" enough to fulfill the "Main Task" and its "Criteria"? If so, stop the main task. If they are not enough, analyze what else is needed to be done to finish the task."""
        if not last_subtask_is_info_extraction_flag:
            final_stop_decision_prompt += """
        - Are all the subtasks in the "Rough Plan" necessary or not? If all of them are unnecessary, you should stop the task. But if there is still at least one necessary subtask in the "Rough Plan", you should continue the task. Analyze the necessity of each subtask(especially the first one) in the "Rough Plan". If there is no subtask in the "Rough Plan", you should stop the task."""
        final_stop_decision_prompt += """
    2. Second, is there any clear advice from the "Node Reflection", for further actions or stopping the task? If there is a clear suggestion about what to do next, you should not stop. But if the advice is not clear or not helpful, you should output "none" to acknowledge that you don't know."""

        if need_answer_flag and ScratchPad_Info.strip() != "":
            final_stop_decision_prompt += """
    3. Third, does the "Observation Description" fulfills the "Criteria"? Is the answer required by the "Criteria" generated in the "Observation" or "ScratchPad Information"? If so, stop the task and you MUST provide the answer directly after the key "final_answer". If not, continue the task to find the answer."""
        final_stop_decision_prompt += """
Finally, summarize your reasoning of the above aspects after the key "reason". Only stop if all conditions are met. Output your final judgment whether to stop the "Main Task" with a True/False response.
        
ATTENTION:
If you decide to stop, the "Main Task" will end immediately with no further actions.
The information above is all you have, with no further additional details.
Do not make assumptions or further inferences(such as "it might be possible" to complete). Judge only based on the information currently available. If the required information from "Criteria" is needed, output it in your response. If you can't, continue the task.

"""
        if True:
            format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_finished_subtasks": string of your reasoning, whether "Finished Subtasks" are enough to fulfill the "Main Task" and its "Criteria";
    "finished_subtasks_sufficient": boolean value, True if "Finished Subtasks" are enough to fulfill the "Main Task" and its "Criteria";"""
            if not last_subtask_is_info_extraction_flag:
                format_instructions_prompt += """
    "reasoning_of_plan": string of your reasoning, analysis of whether each subtask in "Rough Plan" is necessary or not, especially the first one;
    "rough_plan_is_necessary": boolean value, True if at least one subtask in the "Rough Plan" is necessary, False if all the subtasks in the "Rough Plan" are unnecessary, or there is no subtask in the "Rough Plan";"""
            format_instructions_prompt += """
    "reasoning_of_reflection": string of your reasoning, whether the "Node Reflection" suggests further actions clearly or is unclear, or suggests stopping;
    "reflection_suggests_further_actions": boolean value, True if the "Node Reflection" suggests further actions clearly,"""
            if need_answer_flag and ScratchPad_Info.strip() != "":
                format_instructions_prompt += """    
    "reasoning_of_observation": string of your reasoning, whether the current "Observation" meets the "Criteria" and whether the answer is generated from the "Observation" or "ScratchPad Information", should not stop the "Main Task" if the "Observation" does not meet the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" meets the "Criteria" and the answer is generated from the "Observation" or "ScratchPad Information";"""
            else:
                format_instructions_prompt += """
    "reasoning_of_observation": string of your reasoning, whether the current "Observation" meets the "Criteria", should not stop the "Main Task" if the "Observation" does not meet the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" meets the "Criteria";"""
            format_instructions_prompt += """
    "reason": string of the reason summarizing the above aspects, and the final judgment whether to stop the "Main Task", output True only when all aspects are met;
    "stop_decision": the final judgment whether to stop the "Main Task", output True/False;
}
    """
        prompt = gen_basic_prompt("", domain)
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += criteria_prompt
        prompt += finished_subtasks_prompt
        if plan.strip() != "":
            prompt += rough_plan_prompt
        if ScratchPad_Info.strip() != "":
            prompt += scratchpad_info_prompt.format(ScratchPad_Info=ScratchPad_Info)
        else:
            prompt += empty_ScratchPad_Info
        if terminal_node_reflection.strip() != "": # if the last subtask is "info_extraction", there is no terminal_node_reflection
            prompt += node_reflection_prompt
        prompt += final_stop_decision_prompt
        prompt += format_instructions_prompt

        return prompt
    
    @staticmethod
    def judging(reasonings: dict, need_answer_flag: bool):
        """
        after generating the reasoning, use it to generate the final judging prompt of the boolen flags
        """
        reasonings_str = f"""**reasoning_of_finished_subtasks**: {reasonings["reasoning_of_finished_subtasks"]}
"""
        if "reasoning_of_plan" in reasonings:
            reasonings_str += f"""**reasoning_of_plan**: {reasonings["reasoning_of_plan"]}
"""
#         reasonings_str += f"""**reasoning_of_reflection**: {reasonings["reasoning_of_reflection"]}
# """     
        reasonings_str += f"""**reasoning_of_observation**: {reasonings["reasoning_of_observation"]}
"""
        if need_answer_flag:
            reasonings_str += f"""**reasoning_of_answer**: {reasonings["reasoning_of_answer"]}
"""
        output_flags = """
    "finished_subtasks_sufficient": boolean value, True if "reasoning_of_finished_subtasks" indicates that the finished subtasks are enough to complete the main task;"""
        if "reasoning_of_plan" in reasonings:
            output_flags += """
    "rough_plan_is_necessary": boolean value, True if "reasoning_of_plan" indicates there is at least one necessary subtask in the "Rough Plan" needed to be done, False if it thinks all the subtasks are unnecessary, or there is no subtask left;
    "necessity_of_rough_plan": string, describing why 'rough_plan_is_necessary' is True or False;"""
    #     output_flags += """
    # "reflection_suggests_further_actions": boolean value, True if the "reasoning_of_reflection" indicates that the reflection provides clear advice for further actions, False if it is unclear or not helpful;"""
        output_flags += """
    "observation_meets_criteria": boolean value, True if the "reasoning_of_observation" indicates that the observation satisfies the criteria;
"""
        if need_answer_flag:
            output_flags += """
    "answer_available": boolean value, True if the "reasoning_of_answer" indicates that the needed answer can be generated;
"""
            
        prompt = gen_basic_judging_prompt(reasonings_str, output_flags)

        return prompt

# ==================== gen_webtask_stop_verifier_prompt ====================
class gen_webtask_stop_verifier_prompt:
    criteria_prompt = """### Criteria
Following is the state that should be achieved after correctly completing the task. It will be used to determine whether the task has been successfully completed.
{expectation}
"""
    @staticmethod
    def observation(task_content, domain, expectation, actree, observation_description, ):
        logging.info("===== Currentprompt: gen_webtask_stop_verifier_prompt.observation =====")
        observation_description_prompt = f"""### Observation Description
{observation_description}

"""
        webtask_verifier_instruction_prompt = """### Reasoning Process:
Does the information from "Observation" and "Observation Description" meet the "Target Page Description" in "Criteria"? Pay more attention to "Observation Description" than "Observation".
If yes, explain why. If not, analyze what could be done?

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_observation": string of your reasoning, whether the current "Observation" and "Observation Description" meets the "Target Page Description" in the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" and "Observation Description" meets the "Target Page Description" in the "Criteria";
}
""" 
        prompt = gen_basic_prompt("", domain)
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_webtask_stop_verifier_prompt.criteria_prompt.format(expectation=expectation)
        prompt += webtask_verifier_instruction_prompt
        prompt += format_instructions_prompt

        return prompt
    
    @staticmethod
    def plan(task_content, domain, expectation, finished_subtasks,):
        logging.info("===== Currentprompt: gen_webtask_stop_verifier_prompt.plan =====")
        finished_subtasks_prompt = f"""### Finished Subtasks
You have already finished the following subtasks, with their corresponding completeness:
{finished_subtasks}

"""
        webtask_verifier_instruction_prompt = """### Reasoning Process:
Do the "Finished Subtasks" enough to fulfill the "Main Task" and its "Criteria"? If yes, explain why. 
If not, analyze what else is needed to be done to finish the task. What is the further necessary subtask to be done?

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_plan": string of your reasoning, whether the all the necessary subtasks are completed, whether there are any further necessary subtasks to be done;
    "necessary_subtasks_finished": boolean value, True if all necessary subtasks have been finished and no further necessary subtasks need to be done;
}
"""
        prompt = gen_basic_prompt("", domain)
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_webtask_stop_verifier_prompt.criteria_prompt.format(expectation=expectation)
        prompt += finished_subtasks_prompt
        prompt += webtask_verifier_instruction_prompt
        prompt += format_instructions_prompt

        return prompt
    
    @staticmethod
    def answer(task_content, domain, expectation, actree, ScratchPad_Info, final_answer):
        logging.info("===== Currentprompt: gen_webtask_stop_verifier_prompt.answer =====")
        empty_ScratchPad_Info = """### ScratchPad Information
Currently, there is no information in the ScratchPad. If the "Criteria" needs some answer to be stored, it is possiblly you have not found it.

"""
        final_answer_prompt = f"""### Final Answer
Here is the final answer you have generated:
{final_answer}

"""
        webtask_verifier_instruction_prompt = """### Reasoning Process:
Compare the "Final Answer" with the "Criteria". Consider the information from "Observation", and "ScratchPad Information". 
Does the "Final Answer" meet the "Answer Requirements" in the "Criteria"?
Output your analysis after the key "reasoning_of_answer". Then output your answer after the key "final_answer".

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_answer": string of your reasoning, whether the answer provided is generated properly from the "Observation", "Observation Description", and "ScratchPad Information" and meets the "Criteria";
    "answer_meets_criteria": boolean value, True if the answer meets the "Criteria";
    "final_answer": string of the final answer if the "Criteria" requires an answer;
}
""" 
        prompt = gen_basic_prompt("", domain)
        prompt += observation_prompt.format(actree=actree)
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += gen_webtask_stop_verifier_prompt.criteria_prompt.format(expectation=expectation)
        if ScratchPad_Info.strip() != "":
            prompt += scratchpad_info_prompt.format(ScratchPad_Info=ScratchPad_Info)
        else:
            prompt += empty_ScratchPad_Info
        prompt += final_answer_prompt
        prompt += webtask_verifier_instruction_prompt
        prompt += gen_prior_knowledge_emphasize_prompt(prompt)
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def reasoning(task_content, domain, actree, observation_description, expectation, finished_subtasks, ScratchPad_Info, 
                                        final_answer, need_answer_flag,):
        """
        ask one more time to verify the stop decision
        """
        logging.info("===== Currentprompt: gen_webtask_stop_verifier_prompt =====")

        # special prompts
        criteria_prompt = f"""### Criteria
Here is the criteria for finishing the "Main Task":
{expectation}

Judge how the "Main Task" is completed based on the criteria above.

"""
        observation_description_prompt = f"""### Observation Description
{observation_description}

"""
        finished_subtasks_prompt = f"""### Finished Subtasks
You have already finished the following subtasks, with their corresponding completeness:
{finished_subtasks}

"""
        empty_ScratchPad_Info = """### ScratchPad Information
Currently, there is no information in the ScratchPad. If the "Criteria" needs some answer to be stored, it is possiblly you have not found it, which means you cannot stop the task now.

"""
        if need_answer_flag:
            final_answer_prompt = f"""### Final Answer
Here is the final answer you have generated:
{final_answer}

"""
        webtask_verifier_instruction_prompt = """### Reasoning Process:
**Stop Decision**:
Judge whether to stop the current task. Consider the following aspects:
    1. First, do the information from "Observation" and "Observation Description" meet the "Criteria"? Pay more attention to "Observation Description" than "Observation". If not, you should not stop the task.
    2. Second, if the information from observation is not enough, does the "Criteria" require some specific subtasks to be finished? If all necessary subtasks have been finished, what else subtask must be done? If no further necessary subtasks are found, you can stop the task.
"""
        if need_answer_flag:
            webtask_verifier_instruction_prompt += """  3. If there is "Answer Requirements" in the "Criteria", compare the "Final Answer" with the "Criteria". Consider the information from "Observation", "Observation Description", and "ScratchPad Information". If the "Final Answer" meets the "Criteria", you can stop the task.
"""
        webtask_verifier_instruction_prompt += """Evaluate the current situation: Should you stop now? Summarize your reasoning based on the above aspects after the key "reason". Then, indicate whether to halt the "Main Task" with a True/False response.
    
ATTENTION:
If you decide to stop, the "Main Task" will end immediately with no further actions.
The information above is all you have, with no further additional details.
Therefore, do not make assumptions or further inferences(such as "it might be possible" to complete). Judge only based on the information currently available. If the required information from "Criteria" is needed, output it in your response. If you can't, then you should continue the task.

"""
        format_instructions_prompt = """Format your response in JSON, including the following keys:
{
    "reasoning_of_observation": string of your reasoning, whether the current "Observation" meets the "Criteria", whether the answer can be found in the "Observation", should not stop the "Main Task" if the "Observation" does not meet the "Criteria";
    "observation_meets_criteria": boolean value, True if the "Observation" meets the "Criteria";
    "reasoning_of_plan": string of your reasoning, whether the all the necessary subtasks are completed, whether there are any further necessary subtasks, should not stop the "Main Task" if there are further necessary subtasks;
    "necessary_subtasks_finished": boolean value, True if all necessary subtasks have been finished and no further necessary subtasks need to be done;"""
        if need_answer_flag:
            format_instructions_prompt += """
    "reasoning_of_answer": string of your reasoning, whether the answer provided is generated properly from the "Observation", "Observation Description", and "ScratchPad Information" and meets the "Criteria", should not stop if you cannot find the answer;
    "answer_meets_criteria": boolean value, True if the answer meets the "Criteria";
    "final_answer": string of the final answer if the "Criteria" requires an answer."""
        format_instructions_prompt += """
    "reason": string of your summary of the above aspects, and the final judgment whether to stop the "Main Task", output True only when all aspects are met;
    "stop_decision": boolean value of whether to stop the "Main Task".
}
""" 
        prompt = gen_basic_prompt("", domain)
        prompt += observation_prompt.format(actree=actree)
        prompt += observation_description_prompt
        prompt += maintask_prompt.format(task_content=task_content)
        prompt += criteria_prompt
        prompt += finished_subtasks_prompt
        if ScratchPad_Info.strip() != "":
            prompt += scratchpad_info_prompt.format(ScratchPad_Info=ScratchPad_Info)
        else:
            prompt += empty_ScratchPad_Info
        if need_answer_flag:
            prompt += final_answer_prompt
        prompt += webtask_verifier_instruction_prompt
        prompt += format_instructions_prompt

        return prompt

    @staticmethod
    def judging(reasonings: dict, need_answer_flag: bool):
        """
        after generating the reasoning, use it to generate the final judging prompt of the boolen flags
        """
        reasonings_str = f"""**reasoning_of_observation**: {reasonings["reasoning_of_observation"]}
**reasoning_of_plan**: {reasonings["reasoning_of_plan"]}"""
        if need_answer_flag:
            reasonings_str += f"""**reasoning_of_answer**: {reasonings["reasoning_of_answer"]}"""
        reasonings_str += "\n"

        output_flags = """
    "observation_meets_criteria": boolean value, True if the "reasoning_of_observation" indicates that the observation satisfies the criteria;
    "necessary_subtasks_finished": boolean value, True if the "reasoning_of_plan" indicates that all necessary subtasks have been finished and no further necessary subtasks need to be done;"""
        if need_answer_flag:
            output_flags += """
    "answer_meets_criteria": boolean value, True if the "reasoning_of_answer" indicates that the answer meets the requirements of the criteria;"""
        output_flags += "\n"

        prompt = gen_basic_judging_prompt(reasonings_str, output_flags)

        return prompt    
